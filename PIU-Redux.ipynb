{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score,  make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from scipy import stats\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import optuna\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using CUDA (GPU)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folders\n",
    "output_folder = 'output'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Create separate analysis output folders\n",
    "analysis_output_folder = 'analysis_output'\n",
    "os.makedirs(analysis_output_folder, exist_ok=True)\n",
    "\n",
    "physical_analysis_output_folder = 'analysis_output/physical'\n",
    "os.makedirs(physical_analysis_output_folder, exist_ok=True)\n",
    "\n",
    "fitness_analysis_output_folder = 'analysis_output/fitness'\n",
    "os.makedirs(fitness_analysis_output_folder, exist_ok=True)\n",
    "\n",
    "bia_analysis_output_folder = 'analysis_output/bia'\n",
    "os.makedirs(bia_analysis_output_folder, exist_ok=True)\n",
    "\n",
    "child_info_analysis_output_folder = 'analysis_output/child_info'\n",
    "os.makedirs(child_info_analysis_output_folder, exist_ok=True)\n",
    "\n",
    "actigraphy_analysis_output_folder = 'analysis_output/actigraphy'\n",
    "os.makedirs(actigraphy_analysis_output_folder, exist_ok=True)\n",
    "\n",
    "# Set display all columns in dataframes property\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Supress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classes\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(out_features, out_features),\n",
    "            nn.BatchNorm1d(out_features)\n",
    "        )\n",
    "        \n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features)\n",
    "        ) if in_features != out_features else nn.Identity()\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.block(x) + self.skip(x))\n",
    "\n",
    "class AdvancedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=32, dropout_rate=0.2):\n",
    "        super(AdvancedAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder with residual connections and batch normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # First residual block\n",
    "            ResidualBlock(128, 96),\n",
    "            \n",
    "            # Second residual block\n",
    "            ResidualBlock(96, 64),\n",
    "            \n",
    "            # Bottleneck layers with variational component\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            # Mean and log variance for variational component\n",
    "            nn.Linear(32, latent_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder with transpose of encoder architecture\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # Expanding layers\n",
    "            ResidualBlock(32, 64),\n",
    "            ResidualBlock(64, 96),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(96, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=1)\n",
    "        \n",
    "        # Reparameterize\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decode\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        return reconstructed, mu, log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def train_model(model, train_loader, val_loader, epochs=150, learning_rate=0.001, beta=0.05, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    reconstruction_criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed, mu, log_var, z = model(x)\n",
    "            \n",
    "            # Calculate losses\n",
    "            recon_loss = reconstruction_criterion(reconstructed, x)\n",
    "            kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "            total_loss = recon_loss + beta * kl_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += total_loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch[0].to(device)\n",
    "                reconstructed, mu, log_var, z = model(x)\n",
    "                \n",
    "                recon_loss = reconstruction_criterion(reconstructed, x)\n",
    "                kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                total_loss = recon_loss + beta * kl_loss\n",
    "                val_loss += total_loss.item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, history\n",
    "\n",
    "def encode_data(model, data, scaler, device='cuda'):\n",
    "    model.eval()\n",
    "    scaled_data = scaler.transform(data)\n",
    "    data_tensor = torch.FloatTensor(scaled_data).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, mu, _, z = model(data_tensor)\n",
    "    \n",
    "    return mu.cpu().numpy(), z.cpu().numpy()\n",
    "\n",
    "def decode_data(model, latent_vectors, scaler, device='cuda'):\n",
    "    model.eval()\n",
    "    latent_tensor = torch.FloatTensor(latent_vectors).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructed = model.decoder(latent_tensor)\n",
    "    \n",
    "    return scaler.inverse_transform(reconstructed.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_timeseries_autoencoder(df_train, df_test, verbose=True):\n",
    "    \"\"\"Train autoencoder on time series statistics\"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if verbose:\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model\n",
    "    input_dim = df_train.shape[1]\n",
    "    model = AdvancedAutoencoder(input_dim=input_dim, latent_dim=32, dropout_rate=0.2)\n",
    "    \n",
    "    # Prepare data\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(df_train)\n",
    "    test_scaled = scaler.transform(df_test)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    train_tensor = torch.FloatTensor(train_scaled)\n",
    "    test_tensor = torch.FloatTensor(test_scaled)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TensorDataset(train_tensor), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(test_tensor), batch_size=64)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=150,\n",
    "        learning_rate=0.001,\n",
    "        beta=0.05,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Generate encodings\n",
    "    train_mu, train_z = encode_data(model, df_train, scaler, device)\n",
    "    test_mu, test_z = encode_data(model, df_test, scaler, device)\n",
    "    \n",
    "    # Create DataFrames with encoded features\n",
    "    train_encoded = pd.DataFrame(\n",
    "        train_z,\n",
    "        columns=[f'ae_feature_{i}' for i in range(train_z.shape[1])]\n",
    "    )\n",
    "    test_encoded = pd.DataFrame(\n",
    "        test_z,\n",
    "        columns=[f'ae_feature_{i}' for i in range(test_z.shape[1])]\n",
    "    )\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    train_reconstructed = decode_data(model, train_z, scaler, device)\n",
    "    test_reconstructed = decode_data(model, test_z, scaler, device)\n",
    "    \n",
    "    train_mse = np.mean((df_train.values - train_reconstructed) ** 2, axis=1)\n",
    "    test_mse = np.mean((df_test.values - test_reconstructed) ** 2, axis=1)\n",
    "    \n",
    "    train_encoded['reconstruction_error'] = train_mse\n",
    "    test_encoded['reconstruction_error'] = test_mse\n",
    "    \n",
    "    if verbose:\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(history['train_loss'], label='Training Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Autoencoder Training History')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot reconstruction error distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(train_mse, bins=50, alpha=0.5, label='Train')\n",
    "        plt.hist(test_mse, bins=50, alpha=0.5, label='Test')\n",
    "        plt.xlabel('Reconstruction Error')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Reconstruction Error Distribution')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot feature importance\n",
    "        corr = np.corrcoef(train_z.T)[0]\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(range(len(corr)), np.abs(corr))\n",
    "        plt.xlabel('Latent Dimension')\n",
    "        plt.ylabel('Absolute Correlation with First Component')\n",
    "        plt.title('Latent Space Feature Importance')\n",
    "        plt.show()\n",
    "    \n",
    "    return model, train_encoded, test_encoded, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "sample = pd.read_csv('input/sample_submission.csv')\n",
    "\n",
    "print(\"Loading time series data...\")\n",
    "train_ts = load_time_series(\"input/series_train.parquet\")\n",
    "test_ts = load_time_series(\"input/series_test.parquet\")\n",
    "\n",
    "print(\"Preparing features...\")\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "print(\"Training autoencoder...\")\n",
    "model, train_encoded, test_encoded, scaler = train_timeseries_autoencoder(\n",
    "    df_train, \n",
    "    df_test,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add encoded features to original dataframes\n",
    "print(\"Adding encoded features...\")\n",
    "train_ts_with_features = pd.concat([train_ts, train_encoded], axis=1)\n",
    "test_ts_with_features = pd.concat([test_ts, test_encoded], axis=1)\n",
    "\n",
    "print(\"\\nTrain shape after encoding:\", train_ts_with_features.shape)\n",
    "print(\"Test shape after encoding:\", test_ts_with_features.shape)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skew removal for specified columns\n",
    "skewed_columns = [\n",
    "    'BIA-BIA_BMC', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_Fat',\n",
    "    'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', \n",
    "    'BIA-BIA_TBW', 'CGAS-CGAS_Score', 'stat_23', 'stat_35', 'stat_38', 'stat_40', 'stat_47',\n",
    "    'stat_54', 'stat_66', 'stat_78', 'stat_80', 'stat_88', 'stat_90'\n",
    "]\n",
    "\n",
    "def box_cox_transform(df, column, lambda_param=None, verbose=True):\n",
    "    \"\"\"Apply Box-Cox transformation to a column\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.dropna(subset=[column])\n",
    "    \n",
    "    # Ensure all values are positive\n",
    "    min_value = df_copy[column].min()\n",
    "    if min_value <= 0:\n",
    "        df_copy[column] = df_copy[column] - min_value + 1\n",
    "    \n",
    "    try:\n",
    "        if lambda_param is None:\n",
    "            transformed_values, lambda_param = stats.boxcox(df_copy[column])\n",
    "            if verbose:\n",
    "                print(f\"Optimal lambda for {column}: {lambda_param:.4f}\")\n",
    "        else:\n",
    "            transformed_values = stats.boxcox(df_copy[column], lmbda=lambda_param)\n",
    "            if verbose:\n",
    "                print(f\"Applied lambda {lambda_param:.4f} to {column}\")\n",
    "        \n",
    "        df_copy[f'{column}_boxcox'] = transformed_values\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Rows before/after: {len(df)}/{len(df_copy)}\")\n",
    "            \n",
    "        return df_copy, lambda_param\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error transforming {column}: {str(e)}\")\n",
    "        return df_copy, None\n",
    "\n",
    "def replace_inf_with_max(df):\n",
    "    \"\"\"Replace infinite values with maximum non-infinite value\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for column in df_copy.columns:\n",
    "        if df_copy[column].dtype == 'float64':\n",
    "            mask = ~np.isinf(df_copy[column])\n",
    "            if mask.any():\n",
    "                max_value = df_copy.loc[mask, column].max()\n",
    "                df_copy[column] = df_copy[column].replace([np.inf, -np.inf], max_value)\n",
    "    return df_copy\n",
    "\n",
    "def process_skewed_columns(train_df, test_df, skewed_columns, verbose=True):\n",
    "    \"\"\"Process skewed columns in both train and test data\"\"\"\n",
    "    lambda_params = {}\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    \n",
    "    # Process training data and store lambda values\n",
    "    print(\"Processing training data...\")\n",
    "    for column in tqdm(skewed_columns):\n",
    "        if column in train_df.columns:\n",
    "            transformed_train, lambda_params[column] = box_cox_transform(\n",
    "                train_df, \n",
    "                column, \n",
    "                verbose=verbose\n",
    "            )\n",
    "            if lambda_params[column] is not None:\n",
    "                train_processed[f'{column}_boxcox'] = transformed_train[f'{column}_boxcox']\n",
    "    \n",
    "    # Process test data using stored lambda values\n",
    "    print(\"\\nProcessing test data...\")\n",
    "    for column in tqdm(skewed_columns):\n",
    "        if column in test_df.columns and column in lambda_params:\n",
    "            transformed_test, _ = box_cox_transform(\n",
    "                test_df, \n",
    "                column, \n",
    "                lambda_param=lambda_params[column],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            if lambda_params[column] is not None:\n",
    "                test_processed[f'{column}_boxcox'] = transformed_test[f'{column}_boxcox']\n",
    "    \n",
    "    # Handle infinite values\n",
    "    train_processed = replace_inf_with_max(train_processed)\n",
    "    test_processed = replace_inf_with_max(test_processed)\n",
    "    \n",
    "    # Get list of successfully transformed columns\n",
    "    transformed_columns = [f'{col}_boxcox' for col in skewed_columns \n",
    "                         if f'{col}_boxcox' in train_processed.columns]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nSuccessfully transformed {len(transformed_columns)} columns\")\n",
    "        \n",
    "    return train_processed, test_processed, lambda_params, transformed_columns\n",
    "\n",
    "# Function to visualize transformations\n",
    "def plot_transformations(df_before, df_after, columns, n_cols=3, max_rows=5):\n",
    "    \"\"\"Plot histograms before and after transformation\"\"\"\n",
    "    n_cols = min(n_cols, len(columns))\n",
    "    n_rows = min(max_rows, (len(columns) + n_cols - 1) // n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols * 2, figsize=(n_cols * 8, n_rows * 4))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, column in enumerate(columns[:n_rows * n_cols]):\n",
    "        # Original distribution\n",
    "        sns.histplot(df_before[column], ax=axes[i*2], kde=True)\n",
    "        axes[i*2].set_title(f'Original: {column}')\n",
    "        axes[i*2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Transformed distribution\n",
    "        sns.histplot(df_after[f'{column}_boxcox'], ax=axes[i*2+1], kde=True)\n",
    "        axes[i*2+1].set_title(f'Transformed: {column}')\n",
    "        axes[i*2+1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply transformations\n",
    "print(\"Applying Box-Cox transformations...\")\n",
    "train_processed, test_processed, lambda_params, transformed_cols = process_skewed_columns(\n",
    "    df_train, \n",
    "    df_test, \n",
    "    skewed_columns,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Visualize some transformations\n",
    "print(\"\\nPlotting transformations...\")\n",
    "plot_transformations(df_train, train_processed, \n",
    "                    [col for col in skewed_columns if f'{col}_boxcox' in transformed_cols])\n",
    "\n",
    "# Update training data for autoencoder\n",
    "df_train = train_processed\n",
    "df_test = test_processed\n",
    "\n",
    "print(\"\\nData preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure IDs are properly formatted\n",
    "train_ts_with_features['id'] = train_ts_with_features['id'].astype(str)\n",
    "test_ts_with_features['id'] = test_ts_with_features['id'].astype(str)\n",
    "train['id'] = train['id'].astype(str)\n",
    "test['id'] = test['id'].astype(str)\n",
    "\n",
    "# Get lists of our different feature types\n",
    "encoded_cols = [col for col in train_ts_with_features.columns if 'ae_feature_' in col]\n",
    "boxcox_cols = [col for col in train_processed.columns if '_boxcox' in col]\n",
    "stats_cols = [col for col in train_processed.columns if col not in boxcox_cols + ['id']]  # Keep original stats\n",
    "\n",
    "# Create DataFrame with all features: original stats, box-cox transformations, and encoded features\n",
    "train_all_features = pd.concat([\n",
    "    train_ts_with_features[['id'] + encoded_cols],  # Encoded features\n",
    "    train_processed[boxcox_cols],                   # Box-cox transformations\n",
    "    train_processed[stats_cols]                     # Original stats\n",
    "], axis=1)\n",
    "\n",
    "test_all_features = pd.concat([\n",
    "    test_ts_with_features[['id'] + encoded_cols],   # Encoded features\n",
    "    test_processed[boxcox_cols],                    # Box-cox transformations\n",
    "    test_processed[stats_cols]                      # Original stats\n",
    "], axis=1)\n",
    "\n",
    "# Merge everything with original dataframes\n",
    "print(\"Merging all features with original data...\")\n",
    "train_final = train.merge(\n",
    "    train_all_features,\n",
    "    on='id',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "\n",
    "test_final = test.merge(\n",
    "    test_all_features,\n",
    "    on='id',\n",
    "    how='left',\n",
    "    validate='1:1'\n",
    ")\n",
    "\n",
    "# Verify the merges and feature presence\n",
    "print(\"\\nShape Check:\")\n",
    "print(f\"Original train shape: {train.shape}\")\n",
    "print(f\"Train with all features shape: {train_final.shape}\")\n",
    "print(f\"Original test shape: {test.shape}\")\n",
    "print(f\"Test with all features shape: {test_final.shape}\")\n",
    "\n",
    "# Verify feature counts\n",
    "print(\"\\nFeature Counts:\")\n",
    "print(f\"Number of original stats: {len(stats_cols)}\")\n",
    "print(f\"Number of encoded features: {len(encoded_cols)}\")\n",
    "print(f\"Number of box-cox transformed features: {len(boxcox_cols)}\")\n",
    "print(f\"Total features: {len(stats_cols) + len(encoded_cols) + len(boxcox_cols)}\")\n",
    "\n",
    "# Check that all types of features are present\n",
    "print(\"\\nSample of feature types in final training data:\")\n",
    "print(\"\\nOriginal stats (first 3):\")\n",
    "print(train_final[stats_cols[:3]].head())\n",
    "print(\"\\nEncoded features (first 3):\")\n",
    "print(train_final[encoded_cols[:3]].head())\n",
    "print(\"\\nBox-Cox transformed features (first 3):\")\n",
    "print(train_final[boxcox_cols[:3]].head())\n",
    "\n",
    "# Check for any missing values after merge\n",
    "print(\"\\nMissing Values Check:\")\n",
    "print(\"Train missing values:\")\n",
    "print(train_final.isnull().sum()[train_final.isnull().sum() > 0])\n",
    "print(\"\\nTest missing values:\")\n",
    "print(test_final.isnull().sum()[test_final.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nMerge complete!\")\n",
    "\n",
    "# Print summary of all features\n",
    "print(\"\\nSummary of features:\")\n",
    "feature_types = {\n",
    "    'Original Stats': stats_cols,\n",
    "    'Encoded Features': encoded_cols,\n",
    "    'Box-Cox Transformed': boxcox_cols\n",
    "}\n",
    "for feature_type, features in feature_types.items():\n",
    "    print(f\"\\n{feature_type} ({len(features)}):\")\n",
    "    for feat in features[:5]:  # Show first 5 of each type\n",
    "        print(f\"  - {feat}\")\n",
    "    if len(features) > 5:\n",
    "        print(f\"  ... and {len(features)-5} more\")\n",
    "\n",
    "# Optional: Basic feature analysis\n",
    "print(\"\\nFeature Analysis:\")\n",
    "print(\"\\nMemory usage by feature type:\")\n",
    "for feature_type, features in feature_types.items():\n",
    "    memory = train_final[features].memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "    print(f\"{feature_type}: {memory:.2f} MB\")\n",
    "\n",
    "print(\"\\nCorrelation between original and transformed features:\")\n",
    "for original, transformed in zip(skewed_columns[:3], [col for col in boxcox_cols[:3]]):\n",
    "    if original in train_final.columns:\n",
    "        corr = train_final[original].corr(train_final[transformed])\n",
    "        print(f\"{original} vs {transformed}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement missing data with data from WHO\n",
    "# Load who data and group\n",
    "def load_who_bmi_data(file_path):\n",
    "    who_data = pd.read_csv(file_path)\n",
    "    who_data = who_data.groupby(['age', 'sex']).agg({\n",
    "        'L': 'mean', 'mean_bmi': 'mean', 'S': 'mean'\n",
    "    }).reset_index()\n",
    "    who_data = who_data.set_index(['sex', 'age'])\n",
    "    return who_data\n",
    "\n",
    "def load_who_height_data(file_path):\n",
    "    who_data = pd.read_csv(file_path)\n",
    "    who_data = who_data.groupby(['age', 'sex']).agg({\n",
    "        'mean_height': 'mean'\n",
    "    }).reset_index()\n",
    "    who_data = who_data.set_index(['sex', 'age'])\n",
    "    return who_data\n",
    "\n",
    "who_bmi_data = load_who_bmi_data('supplemental_data/bmi_for_age_5_to_19.csv')\n",
    "who_height_data = load_who_height_data('supplemental_data/height_for_age_5_to_19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions to Impute with data from WHO\n",
    "def get_who_stats(age, sex, data_type='bmi'):\n",
    "    try:\n",
    "        if data_type == 'bmi':\n",
    "            stats = who_bmi_data.loc[(sex, age), ['mean_bmi', 'S']]\n",
    "            return stats['mean_bmi'], stats['S']\n",
    "        elif data_type == 'height':\n",
    "            stats = who_height_data.loc[(sex, age), 'mean_height']\n",
    "            return stats\n",
    "    except KeyError:\n",
    "        return None, None if data_type == 'bmi' else None\n",
    "    \n",
    "def impute_bmi(age, sex):\n",
    "    mean_bmi, sd = get_who_stats(age, sex, 'bmi')\n",
    "    if mean_bmi is not None and sd is not None:\n",
    "        imputed_bmi = np.random.normal(mean_bmi, sd)\n",
    "        return round(imputed_bmi, 8)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def impute_height(age, sex):\n",
    "    mean_height_cm = get_who_stats(age, sex, 'height')\n",
    "    if mean_height_cm is not None:\n",
    "        mean_height_inches = mean_height_cm / 2.54  # Convert cm to inches\n",
    "        return round(mean_height_inches, 2)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def impute_weight(bmi, height_inches):\n",
    "    if bmi is not None and height_inches is not None:\n",
    "        height_meters = height_inches * 0.0254  # Convert inches to meters\n",
    "        weight_kg = bmi * (height_meters ** 2)\n",
    "        weight_lbs = weight_kg * 2.20462  # Convert kg to lbs\n",
    "        return round(weight_lbs, 2)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def impute_waist_circumference(weight, height, age, sex):\n",
    "    if weight is not None and height is not None:\n",
    "        if sex == '0':\n",
    "            waist = (weight * 0.5) + (height * 0.2) - (age * 0.1)\n",
    "        else:\n",
    "            waist = (weight * 0.4) + (height * 0.3) - (age * 0.1)\n",
    "        return round(waist, 2)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def impute_body_fat(bmi, age, sex):\n",
    "    if bmi is not None:\n",
    "        if sex == '0':\n",
    "            body_fat = (1.20 * bmi) + (0.23 * age) - 16.2\n",
    "        else:\n",
    "            body_fat = (1.20 * bmi) + (0.23 * age) - 5.4\n",
    "        return round(max(body_fat, 0), 2)  # Ensure body fat is not negative\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def impute_sds_total_raw():\n",
    "    # Based on the distribution in Image 1\n",
    "    mu, sigma = 40, 12  # Estimated from the graph\n",
    "    imputed_value = np.random.normal(mu, sigma)\n",
    "    return round(max(min(imputed_value, 90), 20))  # Clip between 20 and 90\n",
    "\n",
    "def impute_sds_total_t(sds_total_raw):\n",
    "    if sds_total_raw is not None:\n",
    "        # Rough linear transformation based on the relationship between raw and T scores\n",
    "        t_score = 50 + (sds_total_raw - 40) * (10 / 12)\n",
    "        return round(max(min(t_score, 100), 40))  # Clip between 40 and 100\n",
    "    else:\n",
    "        # If raw score is not available, impute based on the distribution in Image 2\n",
    "        mu, sigma = 60, 15  # Estimated from the graph\n",
    "        imputed_value = np.random.normal(mu, sigma)\n",
    "        return round(max(min(imputed_value, 100), 40))  # Clip between 40 and 100\n",
    "\n",
    "def impute_sii_function(bmi, waist_circumference, body_fat, sds_total_raw, sds_total_t):\n",
    "    # Create a simple scoring system\n",
    "    score = 0\n",
    "    \n",
    "    # BMI-based score\n",
    "    if bmi is not None:\n",
    "        if bmi < 18.5:\n",
    "            score += 1\n",
    "        elif bmi >= 25:\n",
    "            score += 2\n",
    "        elif bmi >= 30:\n",
    "            score += 3\n",
    "    \n",
    "    # Waist circumference-based score (using general guidelines)\n",
    "    if waist_circumference is not None:\n",
    "        if waist_circumference > 60:  # This is a general threshold, might need adjustment\n",
    "            score += 1\n",
    "    \n",
    "    if sds_total_t is not None:\n",
    "        if sds_total_t > 80:\n",
    "            score += 1\n",
    "        if sds_total_t > 90:\n",
    "            score += 1\n",
    "    \n",
    "    # Map the score to sii values without forcing a 3\n",
    "    if score <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def apply_imputation(df, impute_sii=True):\n",
    "    def impute_if_missing(row):\n",
    "        age = row['Basic_Demos-Age']\n",
    "        sex = row['Basic_Demos-Sex']\n",
    "        \n",
    "        if pd.isna(row['Physical-BMI']) and 5 <= age <= 19:\n",
    "            row['Physical-BMI'] = impute_bmi(age, sex)\n",
    "        \n",
    "        if pd.isna(row['Physical-Height']) and 5 <= age <= 19:\n",
    "            row['Physical-Height'] = impute_height(age, sex)\n",
    "        \n",
    "        if pd.isna(row['Physical-Weight']) and row['Physical-BMI'] is not None and row['Physical-Height'] is not None:\n",
    "            row['Physical-Weight'] = impute_weight(row['Physical-BMI'], row['Physical-Height'])\n",
    "        \n",
    "        if pd.isna(row['Physical-Waist_Circumference']):\n",
    "            row['Physical-Waist_Circumference'] = impute_waist_circumference(\n",
    "                row['Physical-Weight'], row['Physical-Height'], age, sex\n",
    "            )\n",
    "        \n",
    "        if pd.isna(row['BIA-BIA_Fat']):\n",
    "            row['BIA-BIA_Fat'] = impute_body_fat(row['Physical-BMI'], age, sex)\n",
    "\n",
    "        if pd.isna(row['SDS-SDS_Total_Raw']):\n",
    "            row['SDS-SDS_Total_Raw'] = impute_sds_total_raw()\n",
    "        \n",
    "        if pd.isna(row['SDS-SDS_Total_T']):\n",
    "            row['SDS-SDS_Total_T'] = impute_sds_total_t(row['SDS-SDS_Total_Raw'])\n",
    "        \n",
    "        # Impute sii if it's missing and impute_sii is True\n",
    "        if impute_sii and pd.isna(row['sii']):\n",
    "            row['sii'] = impute_sii_function(\n",
    "                row['Physical-BMI'],\n",
    "                row['Physical-Waist_Circumference'],\n",
    "                row['BIA-BIA_Fat'],\n",
    "                row['SDS-SDS_Total_Raw'],\n",
    "                row['SDS-SDS_Total_T']\n",
    "            )\n",
    "        \n",
    "        return row\n",
    "    \n",
    "    return df.apply(impute_if_missing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute all values including 'sii' for train data\n",
    "train_final = apply_imputation(train_final, impute_sii=True)\n",
    "\n",
    "# Impute all values except 'sii' for test data\n",
    "test_final = apply_imputation(test_final, impute_sii=False)\n",
    "\n",
    "# Check the results\n",
    "print(\"Number of missing values after imputation:\")\n",
    "print(\"BMI:\", train_final['Physical-BMI'].isna().sum())\n",
    "print(\"Height:\", train_final['Physical-Height'].isna().sum())\n",
    "print(\"Weight:\", train_final['Physical-Weight'].isna().sum())\n",
    "print(\"Waist Circumference:\", train_final['Physical-Waist_Circumference'].isna().sum())\n",
    "print(\"Body Fat Percentage:\", train_final['BIA-BIA_Fat'].isna().sum())\n",
    "print(\"SDS Total Raw:\", train_final['SDS-SDS_Total_Raw'].isna().sum())\n",
    "print(\"SDS Total T:\", train_final['SDS-SDS_Total_T'].isna().sum())\n",
    "\n",
    "print(\"\\nSample of imputed values:\")\n",
    "imputed_sample = train_final[\n",
    "    (train_final['Physical-BMI'].notnull() | train_final['Physical-Height'].notnull() | \n",
    "     train_final['Physical-Weight'].notnull() | train_final['Physical-Waist_Circumference'].notnull() | \n",
    "     train_final['BIA-BIA_Fat'].notnull() | train_final['SDS-SDS_Total_Raw'].notnull() | \n",
    "     train_final['SDS-SDS_Total_T'].notnull()) &\n",
    "    ((train_final['Physical-BMI'].notnull() != train_final['Physical-BMI'].notnull().shift()) |\n",
    "     (train_final['Physical-Height'].notnull() != train_final['Physical-Height'].notnull().shift()) |\n",
    "     (train_final['Physical-Weight'].notnull() != train_final['Physical-Weight'].notnull().shift()) |\n",
    "     (train_final['Physical-Waist_Circumference'].notnull() != train_final['Physical-Waist_Circumference'].notnull().shift()) |\n",
    "     (train_final['BIA-BIA_Fat'].notnull() != train_final['BIA-BIA_Fat'].notnull().shift()) |\n",
    "     (train_final['SDS-SDS_Total_Raw'].notnull() != train_final['SDS-SDS_Total_Raw'].notnull().shift()) |\n",
    "     (train_final['SDS-SDS_Total_T'].notnull() != train_final['SDS-SDS_Total_T'].notnull().shift()))\n",
    "].sample(5)[['Basic_Demos-Age', 'Basic_Demos-Sex', 'Physical-BMI', 'Physical-Height', 'Physical-Weight',\n",
    "              'Physical-Waist_Circumference', 'BIA-BIA_Fat', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T']]\n",
    "print(imputed_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis of imputation results\n",
    "print(\"\\nImputation summary by age:\")\n",
    "age_summary = train_final.groupby('Basic_Demos-Age').agg({\n",
    "    'Physical-BMI': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'Physical-Height': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'Physical-Weight': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'Basic_Demos-Sex': 'count'\n",
    "})\n",
    "print(age_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of imputed vs. original data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=train_final, x='Basic_Demos-Age', y='Physical-BMI', hue='Basic_Demos-Sex', alpha=0.5)\n",
    "plt.title('BMI vs Age (After Imputation)')\n",
    "plt.savefig('analysis_output/bmi_vs_age_imputed.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=train_final, x='Basic_Demos-Age', y='Physical-Height', hue='Basic_Demos-Sex', alpha=0.5)\n",
    "plt.title('Height vs Age (After Imputation)')\n",
    "plt.savefig('analysis_output/height_vs_age_imputed.png')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=train_final, x='Basic_Demos-Age', y='Physical-Weight', hue='Basic_Demos-Sex', alpha=0.5)\n",
    "plt.title('Weight vs Age (After Imputation)')\n",
    "plt.savefig('analysis_output/weight_vs_age_imputed.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Imputation analysis plots saved in the 'analysis_output' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def engineer_features(df, is_train=True):\n",
    "    # Combine all grip strength\n",
    "    df['FGC-FGC_GS'] = df['FGC-FGC_GSD_Zone'] + df['FGC-FGC_GSND_Zone']\n",
    "    \n",
    "    # Combine all sit and reach\n",
    "    df['FGC-FGC_SR'] = df['FGC-FGC_SRL_Zone'] + df['FGC-FGC_SRR_Zone']\n",
    "    \n",
    "    # Create a fitness score by adding the zone fitness data\n",
    "    df['fitness_score'] = df['FGC-FGC_GS'] + df['FGC-FGC_SR'] + df['FGC-FGC_CU_Zone'] + df['FGC-FGC_PU_Zone'] + df['FGC-FGC_TL_Zone']\n",
    "    \n",
    "    # Combine PAQ_A-PAQ_A_Total and PAQ_C-PAQ_C_Total into one column\n",
    "    df['PAQ_Total'] = df['PAQ_A-PAQ_A_Total'].combine_first(df['PAQ_C-PAQ_C_Total'])\n",
    "    \n",
    "    # Reworking of features from other notebook\n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    \n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['Age_Internet_Hours'] = df['PreInt_EduHx-computerinternet_hoursday'] / df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BIA_BMI_Internet_Hours_Age'] = (df['BIA-BIA_BMI'] * df['PreInt_EduHx-computerinternet_hoursday']) / df['Basic_Demos-Age']\n",
    "    \n",
    "    # df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    # df['BFP_BMI_Age'] = df['BIA-BIA_Fat'] / (df['BIA-BIA_BMI']*df['BMI_Age'])\n",
    "    \n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    # df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "   \n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['BMR_BMI'] = df['BIA-BIA_BMR'] / df['Physical-BMI']\n",
    "    df['DEE_BMI'] = df['BIA-BIA_DEE'] / df['Physical-BMI']\n",
    "    df['SMM_BMI'] = df['BIA-BIA_SMM'] / df['Physical-BMI']\n",
    "    \n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['Physical-Weight'] / df['BIA-BIA_TBW']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "    df['Age_Weight'] = df['Basic_Demos-Age'] * df['Physical-Weight']\n",
    "    df['Age_Weight_BMI'] = (df['Basic_Demos-Age'] * df['Physical-Weight']) / df['Physical-BMI']\n",
    "    df['Sex_BMI'] = df['Basic_Demos-Sex'] * df['Physical-BMI']\n",
    "    df['Sex_HeartRate'] = df['Basic_Demos-Sex'] * df['Physical-HeartRate']\n",
    "    \n",
    "    df['Age_WaistCirc'] = df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']\n",
    "    df['Age_WaistCirc_BMI'] = np.log10((df['Basic_Demos-Age'] * df['Physical-Waist_Circumference']) / df['Physical-BMI'])\n",
    "    \n",
    "    df['BMI_FitnessMaxStage'] = df['Physical-BMI'] * df['Fitness_Endurance-Max_Stage']\n",
    "    df['Weight_GripStrengthDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSD']\n",
    "    df['Weight_GripStrengthNonDominant'] = df['Physical-Weight'] * df['FGC-FGC_GSND']\n",
    "    df['HeartRate_FitnessTime'] = df['Physical-HeartRate'] * (df['Fitness_Endurance-Time_Mins'] + df['Fitness_Endurance-Time_Sec'])\n",
    "    df['Age_PushUp'] = df['Basic_Demos-Age'] * df['FGC-FGC_PU']\n",
    "    df['FFMI_Age'] = df['BIA-BIA_FFMI'] * df['Basic_Demos-Age']\n",
    "    df['InternetUse_SleepDisturbance'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']\n",
    "    df['InternetUse_SleepDisturbance_BMI'] = (df['PreInt_EduHx-computerinternet_hoursday'] * df['SDS-SDS_Total_Raw']) / df['Physical-BMI']\n",
    "    df['CGAS_BMI'] = df['CGAS-CGAS_Score'] * df['Physical-BMI']\n",
    "    df['CGAS_FitnessMaxStage'] = df['CGAS-CGAS_Score'] * df['Fitness_Endurance-Max_Stage']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to train data\n",
    "train_final = engineer_features(train_final, is_train=True)\n",
    "\n",
    "# Apply feature engineering to test data\n",
    "test_final = engineer_features(test_final, is_train=False)\n",
    "\n",
    "print(\"Feature engineering completed for both train and test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names as a list\n",
    "column_names = train_final.columns.tolist()\n",
    "\n",
    "train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the physical attribute columns and some contextual columns for analysis\n",
    "physical_columns = [\n",
    "    'Basic_Demos-Enroll_Season', 'BMI_Age', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'DEE_Weight',\n",
    "    'CGAS-Season', 'Physical-Season', 'Physical-BMI', 'Internet_Hours_Age', 'Age_Internet_Hours', \n",
    "    'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "    'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'BMI_Internet_Hours', 'BMR_Weight',\n",
    "    'fitness_score', 'BIA-BIA_Frame_num', 'BIA-BIA_BMI', 'PreInt_EduHx-computerinternet_hoursday',\n",
    "    'BIA_BMI_Internet_Hours_Age', \n",
    "    'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight', 'Age_Weight_BMI', 'Sex_BMI', 'Sex_HeartRate',\n",
    "    'sii'\n",
    "]\n",
    "\n",
    "# Isolate the fitness attributes\n",
    "# Removed columns: 'FGC-FGC_CU' 'FGC-FGC_PU', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_SRL' \n",
    "# 'FGC-FGC_GSND_Zone' 'FGC-FGC_GSND' 'FGC-FGC_GSD' 'FGC-FGC_GSD_Zone' 'Fitness_Endurance-Time_Sec'\n",
    "fitness_columns = [\n",
    "    'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'SMM_Height',\n",
    "    'FGC-Season', 'FGC-FGC_CU_Zone', 'FGC-FGC_SR', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL',\n",
    "    'FGC-FGC_TL_Zone', 'FGC-FGC_GS', 'fitness_score', 'BIA-BIA_BMI', 'Physical-BMI', 'FMI_BFP', 'LST_TBW', 'SMM_Height',\n",
    "    'BIA-BIA_Frame_num', 'PreInt_EduHx-computerinternet_hoursday', 'FFMI_BFP', 'BFP_DEE', 'BMR_Weight',\n",
    "    'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'Age_Weight_BMI', 'Sex_HeartRate',\n",
    "    'sii'\n",
    "]\n",
    "\n",
    "# Isolate the BIA attributes\n",
    "bia_columns = [\n",
    "    'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num', 'DEE_Weight', 'SMM_Height',\n",
    "    'BIA-BIA_BMI', 'fitness_score', 'Physical-BMI', 'PreInt_EduHx-computerinternet_hoursday', \n",
    "    'BMR_BMI', 'DEE_BMI', 'SMM_BMI', 'ICW_TBW', 'sii'\n",
    "]\n",
    "\n",
    "# Isolate the PAQ, PCIAT, and SDS\n",
    "child_info_columns = [\n",
    "    'PreInt_EduHx-computerinternet_hoursday', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total',\n",
    "    'PAQ_Total', 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n",
    "    'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09',\n",
    "    'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', \n",
    "    'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', \n",
    "    'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n",
    "    'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'BIA-BIA_BMI', 'fitness_score',\n",
    "    'Physical-BMI', 'Sex_BMI', 'PCIAT-Season', 'sii'\n",
    "]\n",
    "\n",
    "# Isolate the Actigraphy data\n",
    "# removed columns: 'stat_41', 'stat_42', 'stat_39','stat_92_boxcox' 'stat_0', 'stat_1', 'stat_2', 'stat_3', 'stat_4', 'stat_5', 'stat_6', 'stat_7', \n",
    "# 'stat_8', 'stat_9', 'stat_10','stat_11'\n",
    "actigraphy_columns = [\n",
    "    'stat_12', 'stat_13', 'stat_14', 'stat_15', 'stat_16', 'stat_17', 'stat_18', 'stat_19', 'stat_20',\n",
    "    'stat_21', 'stat_22', 'stat_23_boxcox', 'stat_24', 'stat_25', 'stat_26', 'stat_27', 'stat_28', 'stat_29', 'stat_30',\n",
    "    'stat_31', 'stat_32', 'stat_33', 'stat_34', 'stat_35_boxcox', 'stat_36', 'stat_37', 'stat_38_boxcox', 'stat_40_boxcox',\n",
    "    'stat_43', 'stat_44', 'stat_45', 'stat_46', 'stat_47_boxcox', 'stat_48', 'stat_49', 'stat_50',\n",
    "    'stat_51', 'stat_52', 'stat_53', 'stat_54_boxcox', 'stat_55', 'stat_56', 'stat_57', 'stat_58', 'stat_59', 'stat_60',\n",
    "    'stat_61', 'stat_62', 'stat_63', 'stat_64', 'stat_65', 'stat_66_boxcox', 'stat_67', 'stat_68', 'stat_69', 'stat_70',\n",
    "    'stat_71', 'stat_72', 'stat_73', 'stat_74', 'stat_75', 'stat_76', 'stat_77', 'stat_78_boxcox', 'stat_79', 'stat_80_boxcox',\n",
    "    'stat_81', 'stat_82', 'stat_83', 'stat_84', 'stat_85', 'stat_86', 'stat_87', 'stat_88_boxcox', 'stat_89', 'stat_90_boxcox',\n",
    "    'stat_91', 'stat_93', 'stat_94', 'stat_95', 'PreInt_EduHx-computerinternet_hoursday', \n",
    "    'BIA-BIA_Frame_num', 'SDS-SDS_Total_T', 'BIA-BIA_BMI', 'Physical-BMI', 'sii'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze columns\n",
    "def analyze_column(column):\n",
    "    total_count = len(train_final)\n",
    "    missing_count = train_final[column].isnull().sum()\n",
    "    missing_percentage = (missing_count / total_count) * 100\n",
    "    unique_values = train_final[column].nunique()\n",
    "    \n",
    "    if pd.api.types.is_numeric_dtype(train_final[column]):\n",
    "        mean_value = train_final[column].mean()\n",
    "        median_value = train_final[column].median()\n",
    "        std_dev = train_final[column].std()\n",
    "        min_value = train_final[column].min()\n",
    "        max_value = train_final[column].max()\n",
    "        return {\n",
    "            \"Column\": column,\n",
    "            \"Total Count\": total_count,\n",
    "            \"Missing Count\": missing_count,\n",
    "            \"Missing Percentage\": f\"{missing_percentage:.2f}%\",\n",
    "            \"Unique Values\": unique_values,\n",
    "            \"Data Type\": train_final[column].dtype,\n",
    "            \"Mean\": mean_value,\n",
    "            \"Median\": median_value,\n",
    "            \"Standard Deviation\": std_dev,\n",
    "            \"Minimum\": min_value,\n",
    "            \"Maximum\": max_value\n",
    "        }\n",
    "    else:\n",
    "        top_values = train_final[column].value_counts().head(3).to_dict()\n",
    "        return {\n",
    "            \"Column\": column,\n",
    "            \"Total Count\": total_count,\n",
    "            \"Missing Count\": missing_count,\n",
    "            \"Missing Percentage\": f\"{missing_percentage:.2f}%\",\n",
    "            \"Unique Values\": unique_values,\n",
    "            \"Data Type\": train_final[column].dtype,\n",
    "            \"Top 3 Values\": top_values\n",
    "        }\n",
    "\n",
    "# Physical column profiles        \n",
    "physical_column_profiles = [analyze_column(col) for col in physical_columns]\n",
    "physical_column_profiles_df = pd.DataFrame(physical_column_profiles)\n",
    "\n",
    "# Save column profiles to CSV\n",
    "physical_column_profiles_df.to_csv(os.path.join(physical_analysis_output_folder, 'physical_column_profiles.csv'), index=False)\n",
    "print(f\"Column profiles saved to {os.path.join(physical_analysis_output_folder, 'physical_column_profiles.csv')}\")\n",
    "\n",
    "# Fitness column profiles\n",
    "fitness_column_profiles = [analyze_column(col) for col in fitness_columns]\n",
    "fitness_column_profiles_df = pd.DataFrame(fitness_column_profiles)\n",
    "\n",
    "# Save column profiles to CSV\n",
    "fitness_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'fitness_column_profiles.csv'), index=False)\n",
    "print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'fitness_column_profiles.csv')}\")\n",
    "\n",
    "# BIA column profiles\n",
    "bia_column_profiles = [analyze_column(col) for col in bia_columns]\n",
    "bia_column_profiles_df = pd.DataFrame(bia_column_profiles)\n",
    "\n",
    "# Save column profiles to CSV\n",
    "bia_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'bia_column_profiles.csv'), index=False)\n",
    "print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'bia_column_profiles.csv')}\")\n",
    "\n",
    "# Child info column profiles\n",
    "child_info_column_profiles = [analyze_column(col) for col in child_info_columns]\n",
    "child_info_column_profiles_df = pd.DataFrame(child_info_column_profiles)\n",
    "\n",
    "# Save column profiles to CSV\n",
    "child_info_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'child_info_column_profiles.csv'), index=False)\n",
    "print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'child_info_column_profiles.csv')}\")\n",
    "\n",
    "# Actigraphy info column profiles\n",
    "actigraphy_column_profiles = [analyze_column(col) for col in actigraphy_columns]\n",
    "actigraphy_column_profiles_df = pd.DataFrame(actigraphy_column_profiles)\n",
    "\n",
    "# Save column profiles to CSV\n",
    "actigraphy_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'actigraphy_column_profiles.csv'), index=False)\n",
    "print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'actigraphy_column_profiles.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "# Physical columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(train_final[physical_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data in Physical Attribute Columns')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(physical_analysis_output_folder, 'physical_missing_data_heatmap.png'))\n",
    "plt.close()\n",
    "print(f\"Missing data heatmap saved to {os.path.join(physical_analysis_output_folder, 'physical_missing_data_heatmap.png')}\")\n",
    "\n",
    "# Fitness columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(train_final[fitness_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data in Fitness Attribute Columns')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(fitness_analysis_output_folder, 'fitness_missing_data_heatmap.png'))\n",
    "plt.close()\n",
    "print(f\"Missing data heatmap saved to {os.path.join(fitness_analysis_output_folder, 'fitness_missing_data_heatmap.png')}\")\n",
    "\n",
    "# BIA columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(train_final[bia_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data in BIA Attribute Columns')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(bia_analysis_output_folder, 'bia_missing_data_heatmap.png'))\n",
    "plt.close()\n",
    "print(f\"Missing data heatmap saved to {os.path.join(bia_analysis_output_folder, 'bia_missing_data_heatmap.png')}\")\n",
    "\n",
    "# Child info columns\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(train_final[child_info_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data in Child info Attribute Columns')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(child_info_analysis_output_folder, 'child_info_missing_data_heatmap.png'))\n",
    "plt.close()\n",
    "print(f\"Missing data heatmap saved to {os.path.join(child_info_analysis_output_folder, 'child_info_missing_data_heatmap.png')}\")\n",
    "\n",
    "# Actigraphy info columns\n",
    "plt.figure(figsize=(40, 6))\n",
    "sns.heatmap(train_final[actigraphy_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data in Actigraphy info Attribute Columns')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(actigraphy_analysis_output_folder, 'actigraphy_missing_data_heatmap.png'))\n",
    "plt.close()\n",
    "print(f\"Missing data heatmap saved to {os.path.join(actigraphy_analysis_output_folder, 'actigraphy_missing_data_heatmap.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for physical numeric columns\n",
    "physical_numeric_columns = train_final[physical_columns].select_dtypes(include=[np.number]).columns\n",
    "physical_correlation_matrix = train_final[physical_numeric_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(physical_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric Physical Attributes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(physical_analysis_output_folder, 'physical_correlation_matrix.png'))\n",
    "plt.close()\n",
    "print(f\"Physical correlation matrix saved to {os.path.join(physical_analysis_output_folder, 'physical_correlation_matrix.png')}\")\n",
    "\n",
    "# Correlation matrix for fitness numeric columns\n",
    "fitness_numeric_columns = train_final[fitness_columns].select_dtypes(include=[np.number]).columns\n",
    "fitness_correlation_matrix = train_final[fitness_numeric_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(fitness_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric Fitness Attributes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(fitness_analysis_output_folder, 'fitness_correlation_matrix.png'))\n",
    "plt.close()\n",
    "print(f\"Fitness correlation matrix saved to {os.path.join(fitness_analysis_output_folder, 'fitness_correlation_matrix.png')}\")\n",
    "\n",
    "# Correlation matrix for bia numeric columns\n",
    "bia_numeric_columns = train_final[bia_columns].select_dtypes(include=[np.number]).columns\n",
    "bia_correlation_matrix = train_final[bia_numeric_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(bia_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric BIA Attributes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(bia_analysis_output_folder, 'bia_correlation_matrix.png'))\n",
    "plt.close()\n",
    "print(f\"BIA correlation matrix saved to {os.path.join(bia_analysis_output_folder, 'BIA_correlation_matrix.png')}\")\n",
    "\n",
    "# Correlation matrix for child info numeric columns\n",
    "child_info_numeric_columns = train_final[child_info_columns].select_dtypes(include=[np.number]).columns\n",
    "child_info_correlation_matrix = train_final[child_info_numeric_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(24, 22))\n",
    "sns.heatmap(child_info_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric Child info Attributes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(child_info_analysis_output_folder, 'child_info_correlation_matrix.png'))\n",
    "plt.close()\n",
    "print(f\"BIA correlation matrix saved to {os.path.join(child_info_analysis_output_folder, 'child_info_correlation_matrix.png')}\")\n",
    "\n",
    "# Correlation matrix for actigraphy numeric columns\n",
    "actigraphy_numeric_columns = train_final[actigraphy_columns].select_dtypes(include=[np.number]).columns\n",
    "actigraphy_correlation_matrix = train_final[actigraphy_numeric_columns].corr()\n",
    "\n",
    "plt.figure(figsize=(80, 78))\n",
    "sns.heatmap(actigraphy_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric Actigraphy Attributes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(actigraphy_analysis_output_folder, 'actigraphy_correlation_matrix.png'))\n",
    "plt.close()\n",
    "print(f\"BIA correlation matrix saved to {os.path.join(actigraphy_analysis_output_folder, 'actigraphy_correlation_matrix.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all columns into a single list\n",
    "all_columns = []\n",
    "all_columns.extend(physical_numeric_columns)\n",
    "all_columns.extend(fitness_numeric_columns)\n",
    "all_columns.extend(bia_numeric_columns)\n",
    "all_columns.extend(child_info_numeric_columns)\n",
    "all_columns.extend(actigraphy_numeric_columns)\n",
    "\n",
    "# Function to create and save distribution plot\n",
    "def create_distribution_plot(column, category, output_folder, data):\n",
    "    print(f\"\\nProcessing {category} column: {column}\")  # Print current column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[column].dropna(), kde=True)\n",
    "    plt.title(f'Distribution of {category} {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f'{category}_{column}_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Create a dictionary mapping columns to their categories and output folders\n",
    "column_info = {\n",
    "    'physical': (physical_numeric_columns, physical_analysis_output_folder),\n",
    "    'fitness': (fitness_numeric_columns, fitness_analysis_output_folder),\n",
    "    'bia': (bia_numeric_columns, bia_analysis_output_folder),\n",
    "    'child_info': (child_info_numeric_columns, child_info_analysis_output_folder),\n",
    "    'actigraphy': (actigraphy_numeric_columns, actigraphy_analysis_output_folder)\n",
    "}\n",
    "\n",
    "# Create progress bar for all columns\n",
    "with tqdm(total=len(all_columns), desc=\"Creating distribution plots\") as pbar:\n",
    "    # Process each category\n",
    "    for category, (columns, output_folder) in column_info.items():\n",
    "        for column in columns:\n",
    "            # Update progress description to show current column\n",
    "            pbar.set_description(f\"Processing {category}: {column}\")\n",
    "            \n",
    "            # Create and save plot\n",
    "            create_distribution_plot(column, category, output_folder, train_final)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Optional: Add a small delay to make the progress messages readable\n",
    "            time.sleep(0.1)\n",
    "\n",
    "print(\"\\nAll analyses completed and saved to the 'analysis_output' folder.\")\n",
    "\n",
    "# Print summary of processed columns\n",
    "print(\"\\nSummary of processed columns:\")\n",
    "for category, (columns, _) in column_info.items():\n",
    "    print(f\"\\n{category.upper()} columns processed ({len(columns)}):\")\n",
    "    for col in columns:\n",
    "        print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Convert categorical string columns to numeric categories\n",
    "    \"\"\"\n",
    "    # Identify all season columns and other known categorical columns\n",
    "    season_columns = [col for col in df.columns if 'Season' in col]\n",
    "    categorical_columns = season_columns + [\n",
    "        'Basic_Demos-Sex',\n",
    "        'BIA-BIA_Activity_Level_num',\n",
    "        'BIA-BIA_Frame_num'\n",
    "    ]\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            # Fill missing values with a placeholder\n",
    "            df_processed[col] = df_processed[col].fillna('Missing')\n",
    "            \n",
    "            # Create a mapping for unique values\n",
    "            unique_values = df_processed[col].unique()\n",
    "            mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "            \n",
    "            # Convert to numeric using the mapping\n",
    "            df_processed[col] = df_processed[col].map(mapping).astype(int)\n",
    "            \n",
    "            # Store the mapping for future reference\n",
    "            if not hasattr(handle_categorical_columns, 'mappings'):\n",
    "                handle_categorical_columns.mappings = {}\n",
    "            handle_categorical_columns.mappings[col] = mapping\n",
    "            \n",
    "    return df_processed\n",
    "\n",
    "def check_data_types(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Check data types of all columns and identify potential issues\n",
    "    Returns problematic columns that need conversion\n",
    "    \"\"\"\n",
    "    object_cols = []\n",
    "    numeric_cols = []\n",
    "    problematic_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Check if column should be categorical (contains 'Season' or other categorical indicators)\n",
    "            if 'Season' in col or col in ['Basic_Demos-Sex', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num']:\n",
    "                problematic_cols.append((col, 'categorical'))\n",
    "            else:\n",
    "                # Try converting to numeric\n",
    "                try:\n",
    "                    pd.to_numeric(df[col].dropna().iloc[0])\n",
    "                    problematic_cols.append((col, 'numeric'))\n",
    "                except:\n",
    "                    object_cols.append(col)\n",
    "        else:\n",
    "            numeric_cols.append(col)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total columns: {len(df.columns)}\")\n",
    "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "        print(f\"Object columns: {len(object_cols)}\")\n",
    "        print(f\"Potentially problematic columns: {len(problematic_cols)}\")\n",
    "        if problematic_cols:\n",
    "            print(\"\\nProblematic columns that should be converted:\")\n",
    "            for col, col_type in problematic_cols:\n",
    "                print(f\"- {col}: current={df[col].dtype}, should be={col_type}\")\n",
    "                print(f\"  Sample values: {df[col].dropna().head().tolist()}\")\n",
    "    \n",
    "    return problematic_cols\n",
    "\n",
    "def fix_data_types(df):\n",
    "    \"\"\"\n",
    "    Convert data types of problematic columns to appropriate numeric types\n",
    "    \"\"\"\n",
    "    df_fixed = df.copy()\n",
    "    \n",
    "    # First handle categorical columns\n",
    "    df_fixed = handle_categorical_columns(df_fixed)\n",
    "    \n",
    "    # Then handle numeric columns\n",
    "    cols_to_int = [\n",
    "        'Basic_Demos-Sex', 'BIA-BIA_Activity_Level_num', \n",
    "        'BIA-BIA_Frame_num', 'sii'\n",
    "    ]\n",
    "    \n",
    "    cols_to_float = [\n",
    "        'Physical-BMI', 'Physical-Height', 'Physical-Weight',\n",
    "        'Physical-Waist_Circumference', 'Physical-HeartRate',\n",
    "        'Physical-Diastolic_BP', 'Physical-Systolic_BP',\n",
    "        'BIA-BIA_BMI', 'BIA-BIA_Fat', 'BIA-BIA_FFM', 'BIA-BIA_FFMI',\n",
    "        'BIA-BIA_FMI', 'BIA-BIA_ICW', 'BIA-BIA_ECW', 'BIA-BIA_TBW',\n",
    "        'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_SMM', 'BIA-BIA_BMC',\n",
    "        'BIA-BIA_LST', 'BIA-BIA_LDM'\n",
    "    ]\n",
    "    \n",
    "    # Convert remaining numeric columns\n",
    "    for col in df_fixed.columns:\n",
    "        if col not in cols_to_int and col not in cols_to_float:\n",
    "            try:\n",
    "                df_fixed[col] = pd.to_numeric(df_fixed[col], errors='coerce')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Convert specific columns to integer\n",
    "    for col in cols_to_int:\n",
    "        if col in df_fixed.columns:\n",
    "            df_fixed[col] = df_fixed[col].fillna(-999).astype(int)\n",
    "            if col == 'sii':  # Special handling for target variable\n",
    "                df_fixed[col] = df_fixed[col].replace(-999, np.nan)\n",
    "    \n",
    "    # Convert specific columns to float\n",
    "    for col in cols_to_float:\n",
    "        if col in df_fixed.columns:\n",
    "            df_fixed[col] = df_fixed[col].astype(float)\n",
    "    \n",
    "    return df_fixed\n",
    "\n",
    "# Usage\n",
    "print(\"Checking train data types...\")\n",
    "train_final = fix_data_types(train_final)\n",
    "problematic_train = check_data_types(train_final)\n",
    "\n",
    "print(\"\\nChecking test data types...\")\n",
    "test_final = fix_data_types(test_final)\n",
    "problematic_test = check_data_types(test_final)\n",
    "\n",
    "if not problematic_train and not problematic_test:\n",
    "    print(\"\\nAll data types have been fixed successfully!\")\n",
    "else:\n",
    "    print(\"\\nWarning: Some columns still have incorrect data types!\")\n",
    "    \n",
    "# Print sample of categorical mappings\n",
    "if hasattr(handle_categorical_columns, 'mappings'):\n",
    "    print(\"\\nCategorical mappings:\")\n",
    "    for col, mapping in handle_categorical_columns.mappings.items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        for original, encoded in mapping.items():\n",
    "            print(f\"  {original} -> {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate category columns\n",
    "cat_columns = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', 'PCIAT-Season',\n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def replace_inf_nan(df):\n",
    "    \"\"\"Replace infinite values with NaN and then fill NaN with median for numeric columns\"\"\"\n",
    "    # Replace inf with NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Fill NaN with median for numeric columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_columns:\n",
    "    mapping = create_mapping(col, train_final)\n",
    "    mappingTe = create_mapping(col, test_final)\n",
    "    \n",
    "    train_final[col] = train_final[col].replace(mapping).astype(int)\n",
    "    test_final[col] = test_final[col].replace(mappingTe).astype(int)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train_final.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train_final[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train_final.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train_final[col]\n",
    "        \n",
    "train_final = train_imputed\n",
    "\n",
    "train_final = engineer_features(train_final)\n",
    "train_final = train_final.dropna(thresh=10, axis=0)\n",
    "test_final = engineer_features(test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset output folder\n",
    "output_folder = 'output'\n",
    "\n",
    "# Export train_data to CSV\n",
    "train_output_path = os.path.join(output_folder, 'train_data_imputed.csv')\n",
    "train_final.to_csv(train_output_path, index=False)\n",
    "print(f\"Imputed train data exported to: {train_output_path}\")\n",
    "\n",
    "# Export test_data to CSV\n",
    "test_output_path = os.path.join(output_folder, 'test_data_imputed.csv')\n",
    "test_final.to_csv(test_output_path, index=False)\n",
    "print(f\"Imputed test data exported to: {test_output_path}\")\n",
    "\n",
    "# Make copies of data for other submissions\n",
    "train2 = train_final.copy()\n",
    "test2 = test_final.copy()\n",
    "\n",
    "train3 = train_final.copy()\n",
    "test3 = test_final.copy()\n",
    "\n",
    "print(\"Data export completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_splits = 5\n",
    "\n",
    "# Function definitions\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train_final.drop(['sii'], axis=1)\n",
    "    y = train_final['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "Params = {\n",
    "    'n_estimators': 682,\n",
    "    'learning_rate': 0.022201704131134002,\n",
    "    'max_depth': 3,\n",
    "    'num_leaves': 843,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.8918812900108436,\n",
    "    'bagging_freq': 10,\n",
    "    'lambda_l1': 4.79779460021304e-07,\n",
    "    'lambda_l2': 2.0055171376757653e-06,\n",
    "    'min_child_samples': 39,\n",
    "    'colsample_bytree': 0.9264391369678474\n",
    "}\n",
    "\n",
    "\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.02829495436971426,\n",
    "    'max_depth': 8,\n",
    "    'n_estimators': 484,\n",
    "    'subsample': 0.9834706801888403,\n",
    "    'colsample_bytree': 0.7681852816292032,\n",
    "    'reg_alpha': 0.010495697417466835,\n",
    "    'reg_lambda': 0.022138771647168275,\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'exact',\n",
    "    'min_child_weight': 9\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.09981560408272906,\n",
    "    'depth': 6,\n",
    "    'iterations': 43,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 3.4155853181132496,\n",
    "    'bootstrap_type': 'MVS',\n",
    "    'random_strength': 0.06930624569348895\n",
    "}\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission1 = TrainML(voting_model, test_final)\n",
    "\n",
    "# Save submission\n",
    "Submission1.to_csv('submission_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
